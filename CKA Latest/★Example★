문제1. 『 기본문제 』

1. 쿠버네티스클러스터정보보기
- console에 user 계정으로 로그인 한 후 hk8s 클러스터로 이동하시오.
=> kubectl config use-context hk8s
- hk8s 클러스터의 control-plane, worker node의 이름을 찾아서 /var/CKA2022/hk8s-node-info.txt 파일로 저장하시오.
=> kubectl get nodes | cut -d' ' -f1 | grep -v NAME > /var/CKA2022/hk8s-node-info.txt
- hk8s 클러스터에서 ready상태인 노드의 이름만 추출하여 /var/CKA2022/hk8s-node-ready.txt 파일에 저장하시오.
=> kubectl get nodes | grep -i -w ready | cut -d' ' -f1 > /var/CKA2022/hk8s-node-ready.txt

문제2. 『 기본문제 』

- k8s 클러스터로 이동하시오.
=> kubectl config current-context
=> kubectl config use-context k8s
- k8s 클러스터 상태를 확인합니다.
=> kubectl cluster-info
- k8s 클러스터에서 동작중인 모든 CNI 이름을 /var/CKA2022/k8s_cni_name.txt에 저장하시오
=> ssh k8s-master
=> ls /etc/cni/net.d/
=> exit
=> echo "flannel" > /var/CKA2022/k8s_cni_name.txt
- k8s 클러스터에서 ready 상태인 노드 이름을 추출하여 /var/CKA2022/k8s-node-ready.txt 에 저장하시오.
=> kubectl get nodes | grep -i -w ready | cut -d' ' -f1 > /var/CKA2022/k8s-node-ready.txt

문제3. 『 ETCD Backup & Restore 』

◾ 작업 클러스터: k8s
https://127.0.0.1:2379 에서 실행중인 etcd 의 snapshot을 생성하고 snapshot을 /data/etcd-snapshot.db 에 저장합니다.
그런 다음 /data/etcd-snapshot-previous.db 에 있는 기존의 이전 스냅샷을 복원합니다.
etcdctl을 사용하여 서버에 연결하기 위해 다음 TLS 인증서/키가 제공됩니다.

• CA certificate : /etc/kubernetes/pki/etcd/ca.crt
• Client certificate : /etc/kubernetes/pki/etcd/server.crt
• Client key : /etc/kubernetes/pki/etcd/server.key

sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /data/etcd-snapshot.db

sudo ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-previous(임의설정) /data/etcd-snapshot-previous.db
sudo ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-previous /data/etcd-snapshot-previous.db

*************************************************************************************************
ubuntu@k8s-master:~$ sudo ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /data/etcd-snapshot.db
2023-04-06 17:09:22.639854 I | clientv3: opened snapshot stream; downloading
2023-04-06 17:09:22.828838 I | clientv3: completed snapshot read; closing
Snapshot saved at /data/etcd-snapshot.db
ubuntu@k8s-master:~$ sudo ls -l /data/etcd-snapshot.db
-rw-r--r-- 1 root root 8802336 Apr  6 17:09 /data/etcd-snapshot.db
ubuntu@k8s-master:~$
ubuntu@k8s-master:~$
ubuntu@k8s-master:~$ sudo ETCDCTL_API=3 etcdctl snapshot restore --data-dir /var/lib/etcd-previous /data/etcd-snapshot-previous.db
2023-04-06 17:14:29.826391 I | mvcc: restore compact to 66008
2023-04-06 17:14:29.848350 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
ubuntu@k8s-master:~$ sudo tree /var/lib/etcd-previous/
/var/lib/etcd-previous/
└── member
    ├── snap
    │   ├── 0000000000000001-0000000000000001.snap
    │   └── db
    └── wal
        └── 0000000000000000-0000000000000000.wal

3 directories, 3 files

ubuntu@k8s-master:~$ sudo vi /etc/kubernetes/manifests/etcd.yaml
  - hostPath:
      path: /var/lib/etcd-previous
      type: DirectoryOrCreate
    name: etcd-data
ubuntu@k8s-master:~$ kubectl get pods
NAME                         READY   STATUS                   RESTARTS      AGE
busybox-sleep                1/1     Running                  0             30h
campus-01                    1/1     Running                  0             30h
cka-webserver                1/1     Running                  0             30h
custom-app                   1/1     Running                  0             30h
eshop-cart-app               1/1     Running                  0             30h
fast-01                      1/1     Running                  0             30h
front-end-5b76b7ff75-9cgcb   1/1     Running                  0             30h
front-end-5b76b7ff75-c97hl   0/1     ContainerStatusUnknown   1             30h
front-end-5b76b7ff75-rkns4   1/1     Running                  0             29h
web                          0/1     Completed                1 (30h ago)   30h
******************************************************************************************************

문제 4. 

◾ 작업 클러스터 : k8s
마스터 노드의 모든 Kubernetes control plane 및 node 구성요소를 버전 1.23.3 으로 업그레이드 합니다.
master 노드를 업그레이드 하기전에 drain 하고 업그레이드후에 uncordon 해야합니다.

" Master Node "
1. Master node upgrade 할 
sudo apt update
sudo apt-cache madison kubeadm | head -5
********************************************************************************************************
ubuntu@k8s-master:~$ sudo apt update
Get:1 https://download.docker.com/linux/ubuntu jammy InRelease [48.9 kB]
Hit:3 http://kr.archive.ubuntu.com/ubuntu jammy InRelease
Get:4 http://kr.archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [8993 B]
Get:5 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages [64.8 kB]
Get:6 http://kr.archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]
Get:7 http://kr.archive.ubuntu.com/ubuntu jammy-security InRelease [110 kB]
Get:8 http://kr.archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [993 kB]
Get:9 http://kr.archive.ubuntu.com/ubuntu jammy-updates/main Translation-en [211 kB]
Get:10 http://kr.archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [748 kB]
Get:11 http://kr.archive.ubuntu.com/ubuntu jammy-updates/restricted Translation-en [116 kB]
Get:12 http://kr.archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [899 kB]
Get:13 http://kr.archive.ubuntu.com/ubuntu jammy-updates/universe Translation-en [180 kB]
Get:14 http://kr.archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [24.1 kB]
Get:15 http://kr.archive.ubuntu.com/ubuntu jammy-security/main amd64 Packages [731 kB]
Get:16 http://kr.archive.ubuntu.com/ubuntu jammy-security/main Translation-en [147 kB]
Get:17 http://kr.archive.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [705 kB]
Get:18 http://kr.archive.ubuntu.com/ubuntu jammy-security/restricted Translation-en [110 kB]
Get:19 http://kr.archive.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [716 kB]
Get:20 http://kr.archive.ubuntu.com/ubuntu jammy-security/universe Translation-en [118 kB]
Get:21 http://kr.archive.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [19.4 kB]
Fetched 6176 kB in 6s (1004 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
87 packages can be upgraded. Run 'apt list --upgradable' to see them.

ubuntu@k8s-master:~$ sudo apt-cache madison kubeadm | head -5
   kubeadm |  1.27.0-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.26.3-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.26.2-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.26.1-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubeadm |  1.26.0-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
**********************************************************************************************************************************
 2.
 해당 version 으로 kubeadm upgrade
 sudo -i
 
 apt-mark unhold kubeadm && \
 apt-get update && apt-get install -y kubeadm=1.27.0-00 && \
 apt-mark hold kubeadm
*******************************************************************************************************************
root@k8s-master:~#  apt-mark unhold kubeadm && \
 apt-get update && apt-get install -y kubeadm=1.27.0-00 && \
 apt-mark hold kubeadm
kubeadm was already not on hold.
Hit:1 https://download.docker.com/linux/ubuntu jammy InRelease
Hit:3 http://kr.archive.ubuntu.com/ubuntu jammy InRelease
Hit:2 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Hit:4 http://kr.archive.ubuntu.com/ubuntu jammy-updates InRelease
Hit:5 http://kr.archive.ubuntu.com/ubuntu jammy-backports InRelease
Hit:6 http://kr.archive.ubuntu.com/ubuntu jammy-security InRelease
Reading package lists... Done
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  git-man liberror-perl patch
Use 'apt autoremove' to remove them.
The following packages will be upgraded:
  kubeadm
1 upgraded, 0 newly installed, 0 to remove and 86 not upgraded.
Need to get 9931 kB of archives.
After this operation, 1393 kB of additional disk space will be used.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.27.0-00 [9931 kB]
Fetched 9931 kB in 2s (4963 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 68519 files and directories currently installed.)
Preparing to unpack .../kubeadm_1.27.0-00_amd64.deb ...
Unpacking kubeadm (1.27.0-00) over (1.26.0-00) ...
Setting up kubeadm (1.27.0-00) ...
debconf: unable to initialize frontend: Dialog
debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)
debconf: falling back to frontend: Readline
Scanning processes...
Scanning linux images...

Running kernel seems to be up-to-date.

No services need to be restarted.

No containers need to be restarted.

No user sessions are running outdated binaries.

No VM guests are running outdated hypervisor (qemu) binaries on this host.
kubeadm set on hold.
***********************************************************************************************************
3.
upgrade 한 kubeadm version 확인
************************************************************************************************************
root@k8s-master:~# kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.0", GitCommit:"1b4df30b3cdfeaba6024e81e559a6cd09a089d65", GitTreeState:"clean", BuildDate:"2023-04-11T17:09:06Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}

4. 해당 version 으로 kubeadm upgrade 할 수 있는지 확인
************************************************************************************************************
root@k8s-master:~# kubeadm upgrade plan v1.27.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade] Fetching available versions to upgrade to
[upgrade/versions] Cluster version: v1.26.1
[upgrade/versions] kubeadm version: v1.27.0
[upgrade/versions] Target version: v1.27.0
[upgrade/versions] Latest version in the v1.26 series: v1.26.3
W0412 16:00:44.973743   78716 compute.go:307] [upgrade/versions] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     3 x v1.26.0   v1.26.3

Upgrade to the latest version in the v1.26 series:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.26.1   v1.26.3
kube-controller-manager   v1.26.1   v1.26.3
kube-scheduler            v1.26.1   v1.26.3
kube-proxy                v1.26.1   v1.26.3
CoreDNS                   v1.9.3    v1.10.1
etcd                      3.5.6-0   3.5.7-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.26.3

_____________________________________________________________________

Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':
COMPONENT   CURRENT       TARGET
kubelet     3 x v1.26.0   v1.27.0

Upgrade to the latest stable version:

COMPONENT                 CURRENT   TARGET
kube-apiserver            v1.26.1   v1.27.0
kube-controller-manager   v1.26.1   v1.27.0
kube-scheduler            v1.26.1   v1.27.0
kube-proxy                v1.26.1   v1.27.0
CoreDNS                   v1.9.3    v1.10.1
etcd                      3.5.6-0   3.5.7-0

You can now apply the upgrade by executing the following command:

        kubeadm upgrade apply v1.27.0

_____________________________________________________________________


The table below shows the current state of component configs as understood by this version of kubeadm.
Configs that have a "yes" mark in the "MANUAL UPGRADE REQUIRED" column require manual config upgrade or
resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually
upgrade to is denoted in the "PREFERRED VERSION" column.

API GROUP                 CURRENT VERSION   PREFERRED VERSION   MANUAL UPGRADE REQUIRED
kubeproxy.config.k8s.io   v1alpha1          v1alpha1            no
kubelet.config.k8s.io     v1beta1           v1beta1             no
_____________________________________________________________________
**************************************************************************************************************************
5.
해당 version으로 kubeadm upgrade 적용
**************************************************************************************************************************
root@k8s-master:~# sudo kubeadm upgrade apply v1.27.0
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.27.0"
[upgrade/versions] Cluster version: v1.26.1
[upgrade/versions] kubeadm version: v1.27.0
[upgrade] Are you sure you want to proceed? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
W0412 16:02:45.477672   79227 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0412 16:03:15.362028   79227 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.6" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.27.0" (timeout: 5m0s)...
[upgrade/etcd] Upgrading to TLS for etcd
W0412 16:03:49.581856   79227 staticpods.go:305] [upgrade/etcd] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
W0412 16:03:49.609627   79227 images.go:80] could not find officially supported version of etcd for Kubernetes v1.27.0, falling back to the nearest etcd version (3.5.7-0)
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Renewing etcd-server certificate
[upgrade/staticpods] Renewing etcd-peer certificate
[upgrade/staticpods] Renewing etcd-healthcheck-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/etcd.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-04-12-16-03-48/etcd.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=etcd
[upgrade/staticpods] Component "etcd" upgraded successfully!
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests2300688966"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-04-12-16-03-48/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-apiserver
[upgrade/staticpods] Component "kube-apiserver" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-controller-manager" upgrade
[upgrade/staticpods] Renewing controller-manager.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-controller-manager.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-04-12-16-03-48/kube-controller-manager.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-controller-manager
[upgrade/staticpods] Component "kube-controller-manager" upgraded successfully!
[upgrade/staticpods] Preparing for "kube-scheduler" upgrade
[upgrade/staticpods] Renewing scheduler.conf certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-scheduler.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2023-04-12-16-03-48/kube-scheduler.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[apiclient] Found 1 Pods for label selector component=kube-scheduler
[upgrade/staticpods] Component "kube-scheduler" upgraded successfully!
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upgrade] Backing up kubelet config file to /etc/kubernetes/tmp/kubeadm-kubelet-config3966935279/config.yaml
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

[upgrade/successful] SUCCESS! Your cluster was upgraded to "v1.27.0". Enjoy!

[upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.
***********************************************************************************************************************
6.
console 에서 drain 명령어 적용하여 실행중인 서비스 다운시키기
***********************************************************************************************************************
ubuntu@console:~$ kubectl drain k8s-master --ignore-daemonsets
node/k8s-master cordoned
Warning: ignoring DaemonSet-managed Pods: calico-system/calico-node-qhnfp, calico-system/csi-node-driver-kmlj2, kube-system/kube-proxy-n5c66
evicting pod tigera-operator/tigera-operator-dd6596576-lcssc
evicting pod calico-apiserver/calico-apiserver-5fc7766974-q8btp
evicting pod calico-apiserver/calico-apiserver-5fc7766974-k6qqv
evicting pod calico-system/calico-kube-controllers-7f55d475d-mhc8v
evicting pod calico-system/calico-typha-6d4b5d9d58-cdzqj
pod/calico-typha-6d4b5d9d58-cdzqj evicted
pod/calico-kube-controllers-7f55d475d-mhc8v evicted
pod/tigera-operator-dd6596576-lcssc evicted
pod/calico-apiserver-5fc7766974-k6qqv evicted
pod/calico-apiserver-5fc7766974-q8btp evicted
node/k8s-master drained
*************************************************************************************************************************
7. 
kubelet, kubectl upgrade
**************************************************************************************************************************
ubuntu@k8s-master:~$ sudo -i
root@k8s-master:~# apt-mark unhold kubelet kubectl && \
apt-get update && apt-get install -y kubelet=1.27.0-00 kubectl=1.27.0-00 && \
apt-mark hold kubelet kubectl
Canceled hold on kubelet.
Canceled hold on kubectl.
Hit:1 https://download.docker.com/linux/ubuntu jammy InRelease
Hit:2 https://packages.cloud.google.com/apt kubernetes-xenial InRelease
Hit:3 http://kr.archive.ubuntu.com/ubuntu jammy InRelease
Get:4 http://kr.archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]
Get:5 http://kr.archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]
Get:6 http://kr.archive.ubuntu.com/ubuntu jammy-security InRelease [110 kB]
Fetched 337 kB in 17s (19.6 kB/s)
Reading package lists... Done
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages were automatically installed and are no longer required:
  git-man liberror-perl patch
Use 'apt autoremove' to remove them.
The following packages will be upgraded:
  kubectl kubelet
2 upgraded, 0 newly installed, 0 to remove and 65 not upgraded.
Need to get 29.0 MB of archives.
After this operation, 13.9 MB disk space will be freed.
Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubectl amd64 1.27.0-00 [10.2 MB]
Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubelet amd64 1.27.0-00 [18.8 MB]
Fetched 29.0 MB in 4s (6450 kB/s)
debconf: delaying package configuration, since apt-utils is not installed
(Reading database ... 104022 files and directories currently installed.)
Preparing to unpack .../kubectl_1.27.0-00_amd64.deb ...
Unpacking kubectl (1.27.0-00) over (1.26.0-00) ...
Preparing to unpack .../kubelet_1.27.0-00_amd64.deb ...
Unpacking kubelet (1.27.0-00) over (1.26.0-00) ...
Setting up kubectl (1.27.0-00) ...
Setting up kubelet (1.27.0-00) ...
Scanning processes...
Scanning candidates...
Scanning linux images...

Restarting services...
 systemctl restart multipathd.service packagekit.service polkit.service ssh.service systemd-resolved.service
Service restarts being deferred:
 systemctl restart ModemManager.service
 systemctl restart NetworkManager.service
 /etc/needrestart/restart.d/dbus.service
 systemctl restart networkd-dispatcher.service
 systemctl restart systemd-logind.service
 systemctl restart unattended-upgrades.service
 systemctl restart wpa_supplicant.service

No containers need to be restarted.

No user sessions are running outdated binaries.

No VM guests are running outdated hypervisor (qemu) binaries on this host.
kubelet set on hold.
kubectl set on hold.
***************************************************************************************************************************
8.
kubelet 재실행 및 console 에서 uncordon 명령어 실행하여 서비스 재실행

****************************************************************************************************************************
root@k8s-master:~# sudo systemctl daemon-reload
root@k8s-master:~# sudo systemctl restart kubelet

ubuntu@console:~$ kubectl uncordon k8s-master
node/k8s-master uncordoned

ubuntu@console:~$ kubectl get nodes
NAME          STATUS   ROLES           AGE   VERSION
k8s-master    Ready    control-plane   48d   v1.27.0
k8s-worker1   Ready    <none>          48d   v1.26.0
k8s-worker2   Ready    <none>          48d   v1.26.0
*****************************************************************************************************************************

" Worker Node "
